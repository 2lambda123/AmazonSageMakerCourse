{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost Endpoint - Multiple Models hosted on same instance\n",
    "<h4>Invoke Specific Model</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import os\n",
    "import json\n",
    "\n",
    "import boto3\n",
    "import re\n",
    "from sagemaker import get_execution_role\n",
    "import sagemaker\n",
    "\n",
    "# SDK 2 serializers and deserializers\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to specify the location of each model - relative path is fine\n",
    "# TODO - Update your path here\n",
    "targetModels = [\"xgboost-bikerental-hyper-one-2021-06-28-17-25-58-077/output/model.tar.gz\",\n",
    "                \"xgboost-bikerental-hyper-two-2021-06-28-17-29-46-708/output/model.tar.gz\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a predictor and point to an existing endpoint (note this endpoint has two versions of the model)\n",
    "endpoint_name = 'xgboost-bikerental-hyper'\n",
    "predictor = sagemaker.predictor.Predictor (endpoint_name=endpoint_name)\n",
    "predictor.serializer = CSVSerializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Test Data\n",
    "df_all = pd.read_csv('bike_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to pass an array to the prediction\n",
    "# can pass a numpy array or a list of values [[19,1],[20,1]]\n",
    "arr_test = df_all[df_all.columns[1:]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invoke Model One"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_model = path to the model artifact. For multi-model endpoints, we need to provide the path to the model artifact\n",
    "# Call the first model\n",
    "# Output is a JSON List\n",
    "result = predictor.predict(\n",
    "    arr_test[:5], \n",
    "    target_model=targetModels[0])\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invoke Model Two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_model = path to the model artifact. For multi-model endpoints, we need to provide the path to the model artifact\n",
    "# Call the second model\n",
    "# Output is a JSON List\n",
    "result = predictor.predict(\n",
    "    arr_test[:5], \n",
    "    target_model=targetModels[1])\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the input data into chunks\n",
    "There are thousands of rows in this data set for which need inference.  \n",
    "When communicating over internet, it is a good idea to split the data into chunks to prevent payload and timeout error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_by_version(targetModel = None):\n",
    "    # For large number of predictions, we can split the input data and\n",
    "    # Query the prediction service.\n",
    "    # array_split is convenient to specify how many splits are needed\n",
    "    predictions = []\n",
    "    for arr in np.array_split(arr_test,10):\n",
    "        result = predictor.predict(arr, target_model=targetModel)\n",
    "        result = json.loads(result.decode(\"utf-8\"))\n",
    "        print (arr.shape)\n",
    "        predictions += [float(r) for r in result]\n",
    "        \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use all available variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('model one inference')\n",
    "df_all[\"count_hyper_one\"] = np.expm1(inference_by_version(targetModel=targetModels[0]))\n",
    "print('model two inference')\n",
    "df_all[\"count_hyper_two\"] = np.expm1(inference_by_version(targetModel=targetModels[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all[[\"count_hyper_one\",\"count_hyper_two\"]].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete Endpoint to prevent unnecessary charges\n",
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
